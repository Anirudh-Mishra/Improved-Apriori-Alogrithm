# Improved-Apriori-Alogrithm

We aim at reducing the complexity of the algorithm and enhancing its efficiency by tweaking the intuition of the algorithm. We plan to conduct a detailed analysis and comparison between both the standard apriori algorithm and the proposed improved apriori algorithm and conclude on a positive note with the benefits of our proposed algorithm.<br><br>
The Apriori algorithm uses frequent itemsets to get association rules, and it's designed to figure on the databases that contain transactions. It employs  an  iterative  approach  known  as  a  levelwise search,  where  k-itemsets  are  used  to  explore  (k+1)-itemsets. The process starts with the finding of frequent 1-itemsets by scanning the database and gathering the count of each item in all transactions and then retaining those items having an occurrence greater than the minimum support value chosen. The next step involves finding all possible combinations of the items found in the 1-itemsets to form 2-itemsets, again whose occurrences are checked for in the transactions and are either retained or discarded based on their values with respect to the minimum support value. This is then extended for finding frequent 3-itemsets, which is then extended to find frequent 4-itemsets and so on till finally no k-itemsets can be generated from (k-1)-itemsets. The list of k-itemsets is represented by the variable Lk. Finding each Lk requires a complete scan of the database. The process of scanning the database is first of all really slow, and also consists of many redundancies thus messing up the efficiency of the algorithm massively.
<br><br>
As mentioned above, the major issues in the traditional Apriori algorithm involve multiple scannings of the database and also the time complexity of lookups in the database for particular transactions that are required for each step thus resulting in a huge time complexity rendering it inefficient. 
<br><br>
We aim to improve the overall efficiency of the algorithm by working on these two aspects by the introduction of the hash table data structure into the implementation of this algorithm along with the concept of transaction reduction.  
<br>
Hash table is a data structure known for its high speed and amazing time complexity when it comes to lookups in the data structure for particular elements. It makes use of either a single or multiple functions known as hashing functions that map a certain element to a certain position in the table based on the processing of some property of the element by the hash function. This makes it easier for one to locate a particular element as well since all it requires is passing that element through that very function to obtain its exact position in the structure. Therefore, we try to improve the lookup time for particular items in the database which might be required for forming itemsets with the help of this data structure. 
<br><br>
The other concept that we are using involves transaction reduction. As the name suggests, we focus on reducing the number of transactions being scanned in each iteration for the formation of itemsets. We introduce a variable that keeps track of the size of transactions and another threshold value, in addition to the minimum support value, that helps in discarding smaller transactions(consisting of fewer items) as the probability of smaller transactions forming a frequent itemset is almost negligible. This reduces the number of elements required for scanning in each transaction thus improving the time complexity by a massive extent due to the repetitive reduction of steps in each iteration. 
